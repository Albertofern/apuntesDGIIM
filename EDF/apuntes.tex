
\section{Primeras definiciones}

En esta asignatura estudiaremos ecuaciones diferenciales, es decir, ecuaciones que relacionan una función $x \in \mathcal{C}^k(I)$
con sus derivadas. Para formalizar este concepto, vamos a dar algunas definiciones.

\begin{ndef}[Dominio]
  Sea $D \subseteq \R^N$. Diremos que es un dominio si es abierto y conexo.
\end{ndef}

Ya sabemos que un dominio, por ser abierto y conexo, es conexo por arcos.

\begin{ndef}[Ecuación diferencial]
  Sean $D \subseteq \R^{k+2}$ un dominio y $\upphi : D \to \R$ continua.

  Entonces la expresión
  \[\upphi\left(t, x(t), x^{(1)}(t), \dots, x^{(k)}(t)\right) = 0\]
  es una ecuación diferencial de orden $k$.
\end{ndef}


\begin{ndef}[Solución de una ecuación diferencial]
  Sean 
  \begin{itemize}
  \item $\upphi : D \subseteq \R^{k+2} \to \R$ una función de una ecuación diferencial
  \item $I \subseteq \R$ un intervalo abierto
  \item $x \in \mathcal{C}^k(I)$ tal que $(t, x(t), x^{(1)}(t), \dots, x^{(k)}(t)) \in D$
  \end{itemize}

  Entonces, $x$ es solución de la ecuación diferencial dada por $\upphi$ si

  \[
    \upphi(t, x(t), x^{(1)}(t), \dots, x^{(k)}(t)) = 0 \ \forall t \in I
  \]
\end{ndef}

\begin{ndef}[Problema de valores iniciales]
  Un problema de valores iniciales es un sistema de ecuaciones

  \[
  \begin{cases}
    \upphi\left(t, x(t), x^{(1)}(t), \dots, x^{(k)}(t)\right) = 0 \\
    x(t_0) = x_0
  \end{cases}
  \]

  Donde $\upphi$ determina una ecuación diferencial y $t_0$ y $x_0$ son dados.
\end{ndef}

\section{Ecuaciones diferenciales de primer orden}

\begin{ndef}[Forma normal de una ecuación diferencial de primer orden]
  Si $\upphi : D \subseteq \R^{3} \to \R$ determina una ecuación diferencial de primer orden y existe $f : \R^2 \to \R$ tal que

  \[
  \begin{array}{ll}
    \upphi\left(t,x(t),x'(t)\right) = 0 & (1) \iff \\
    x' = f\left(t,x\right) & (2)
  \end{array}
  \]

  Se dice que (2) es la forma normal de (1).
  
\end{ndef}


Vamos a ver algunos ejemplos de ecuaciones diferenciales de primer orden y soluciones suyas (marcadas con subíndices).

\begin{ejemplo}
\[x(t)^2 + x'(t)^2 = 4 \]
\[ \upphi(t,x,y) = x^2+y^2 -4\]
Soluciones:
\[ 
\begin{array}{ll}
  x_1(t) = 2 & x_2(t) = -2 \\
  x_3(t) = \sqrt{2}\sen t & x_4(t) = \sqrt{2}\cos t \\
  x_5(t) = \sqrt{2}\cos(t+\alpha) \text{ con } \alpha\in\R
\end{array}
\]
\end{ejemplo}

\begin{ejemplo}

\[ x'(t) = 7x(t) \]
\[ \upphi(t,x,y) = y - 7x \]
Soluciones:
\[ x_1(t) = e^{7t}, \quad  x_2(t) = ke^{7t} \text{ con } k \in \R \] 


\end{ejemplo}


\begin{ejemplo}
La ecuación $x(t)x'(t) = 1$ está dada por $\upphi(t,x,y) = xy - 1$. ¿Es $\varphi(t) = \sqrt{2t+1}$ una solución de la ecuación?

$\varphi$ está definida en $[-1/2, \infty)$ y es derivable en $I = (-1/2, \infty)$

\begin{itemize}
\item $I$ es abierto de $\R$
\item $\exists \varphi'(t) = \frac{1}{\sqrt{2t+1}}, \forall t \in I$
\item $(t, \varphi(t), \varphi'(t)) \in D = \R^3, \forall t \in I$
\item $\upphi\left(t, \varphi(t), \varphi'(t)\right) = 0$
\end{itemize}

Comprobamos que por tanto que es solución.

También se puede despejar $x'$ de la ecuación para hallar la forma normal:

\[ x' = \frac{1}{x} \implies \upphi(t, x, y) = y-\frac{1}{x} \]

Y podemos elegir entonces dos dominios distintos:

\[ 
\begin{array}{l}
  D_1 = \R \times (0,+\infty )\times \R \\
  D_2 = \R \times (-\infty ,0)\times \R
\end{array}
\]

Nótese que al dividir por $x$, hemos eliminado la solución cero, pero como no era solución son la misma ecuación.


\end{ejemplo}

\subsection{Ecuación de crecimiento constante}

Dedicaremos esta sección a una clase de ecuaciones particular, aquellas que son de la forma

\[ x' = kx,\ \ k\in \R \ \ (E) \]

Estas ecuaciones son ecuaciones diferenciales de primer orden, y en particular, si vienen dadas en esta forma, están en forma normal.

Las soluciones a esta ecuación tienen una interpretación en álgunos ámbitos, donde $x(t)$ es una cantidad o proporción de población en el instante $t$ respecto de algún tiempo inicial.\\


\begin{nth}
  Sean $I \subseteq \R$ un intervalo abierto y $\varphi(t)$ una solución de $(E)$, entonces,
  \[\varphi (t) = Ae^{kt} \forall t \in I\]

  para algún $A\in \R$.

\end{nth}

\begin{proof}
  Sea $\varphi(t)$ solución de $x'(t) = kx(t)$ definida en $I$. Para cada $t\in I$ considero $e^{-kt}\varphi (t)$, que es derivable.
  
  \[
  \begin{array}{l}
    (e^{-kt}\varphi (t))' = -ke^{-kt}\varphi (t) + e^{-kt}\varphi' (t) = -ke^{-kt}\varphi (t) + e^{-kt}k\varphi (t) = e^{-kt}\varphi (t) (-k+k) = 0, \forall t \in I
  \end{array}
  \]

  Por tanto

  \[
      e^{-kt}\varphi(t) = A, \ A\in \R \implies \varphi(t) = Ae^{kt}
  \]
  
\end{proof}


\section{Variables separadas}

Consideramos la ecuación

\[
x' = f(t)g(x)
\]

con $\ f:(a,b)\to \R \ $ y $\ g:(c,d) \to \R$ continuas.

Sabemos que $x(t) = k$, $k \in \R$ es una familia de soluciones, para las que $x' = 0$. Ahora, si suponemos que $x$
no es constante, vamos a dar condiciones bajo las cuales la solución a un problema de valores iniciales dado por esa
ecuación es única.

Suponemos además que $g(x) \neq 0 \ \forall x\in (c,d)$.

Sea $x(t)$ solución de la ecuación $\implies x:I \to \R$ tal que $I\subset (a,b),$ con $ x(t)\in (c,d), \forall t \in I$

Entonces $\dfrac{x'(t)}{g(x(t))} = f(t), \forall t \in I$

Fijado $t_0 \in I$, tal que $x(t_0) = x_0$

\[\int_{t_0}^{t}\frac{x'(t)}{g(x(t))} ds = \int_{t_0}^{t} f(s) ds, \forall t \in I \implies G(x(t)) - G(x(t_0)) = \int_{t_0}^{t}f(s) ds\]

Donde $G$ es una primitiva de $\dfrac{1}{g}$. $G'(u) = \dfrac{1}{g(u)}$ tiene signo constante, luego $G$ es estrictamente monótona. Por el teorema de la función inversa, existe $G^{-1}$ y por tanto:

\[x(t) = G^{-1}\left(G(x_0) + \int_{t_0}^{t}f(s) ds\right), \forall t \in I\]

Por tanto, dado un problema de valores iniciales para una ecuación de esta forma, con $g$ no nula en ningún punto, las soluciones son de esta forma. Veamos ahora la unicidad de la solución.

\begin{nth}
  Sea $f\in C(a,b), g\in C(c,d)$, con $g(u) \neq 0, \forall u \in (c,d)$. Entonces dado $t_0 \in (a,b), x_0 \in (c,d)$, existe una única solución de $x' = f(t)g(x)$ que cumple que $x(t_0) = x_0$.
  
Si $x_1$ es otra solución $\implies x_1 = x|_{I_1}$ con $I_1\subset I$.

\end{nth}

% TODO: revisar prueba.


\begin{proof}

\[g(u) \neq 0 \ \forall u \in (c,d) \ \implies \ \frac{1}{g} \in C(c,d) \ \implies \ G \text{ una primitiva de } \frac{1}{g}, \ G\in C^{1}(c,d)\]

\[G'(u) = \frac{1}{g(u)} \neq 0, \ \forall u \in (c,d) \ \implies \ \exists G^{-1}:V\subset \R \to \R, \ G^{-1} \text{ derivable}\]

Con $V$ un abierto y $G(x_0) \in V$.
Sean $x_0 \in (c,d)$ y $\ F(t) = \displaystyle\int_{t_0}^t f(s) ds$, $\ F(t_0) = 0$.

Como $V$ es abierto y $F$ es continua, existe $\tilde{I}$ tal que

\[t \mapsto G(x_0)+\int_{t_0}^t f(s) ds  \in V \text{ cuando } t,t_0 \in \tilde{I}\]

Por tanto, $G^{-1}\left(G(x_0) + F(t)\right), \ \forall t \in I$ está bien definida y por el teorema de la función inversa:

\[x'(t) = \frac{1}{G'(G^{-1}(G(x_0) + F(t)))}\]


\end{proof}

  
\section{Cambio de variable en ecuaciones diferenciales}
El objetivo del cambio de variable será transformar una ecuación en una más fácilmente resoluble.
Vamos a estudiar el caso de las ecuaciones diferenciales de primer orden.

% TODO: definir difeomorfismo. ¿En esta sección?

Consideramos una ecuación de este tipo en forma normal:

\[
  x' = F(t, x),\ F : D \subseteq \R^2 \to \R \ \ \ \ (E)
  \]

  y un difeomorfismo

  \[
  \begin{array}{lll}
    \varphi : & D \to \tilde{D} \\ 
  \end{array}
  \]

  de forma que, si $\tilde{F} = F \circ \varphi$, la ecuación
  \[ \dfrac{dy}{ds} = \tilde{F}(s, y) \ \ \ \ (\tilde{E})\]
  es equivalente a la que estamos considerando.


\begin{nprop}

Dado $\varphi = (\varphi _1, \varphi _2) : D \to \tilde{D}$ un difeomorfismo, tal que 
\[
\frac{d\varphi_1}{dt} = \dfrac{\partial \varphi_1}{\partial t}(t, x(t)) + \dfrac{\partial \varphi_1}{\partial x}(t, x(t)) F(t,x(t)) \neq 0 \ \forall (t,x) \in D
\]

Entonces el cambio de variable:
\[
\begin{array}{l}
  s := \varphi _1(t,x)\\
  y := \varphi _2 (t,x)
\end{array}
\]
transforma (E) en otra equivalente $(\tilde{E}) \ \ \ \ \frac{dy}{ds} = \tilde{F}(s,y)$ en el sentido de que para cualquier solución $x = x(t)$ de (E) definida en un intervalo I, existe una función $y=y(s)$ solución de ($\tilde{E}$) definida en un intervalo $J$ y $\varphi(t, x(t)) = (s(t), \ \ y(s(t))) \ \forall t \in I$ y reciprocamente.
\end{nprop}

\begin{proof}

  De una parte, sabemos que $\dfrac{dy}{ds} = \dfrac{dy}{dt}\dfrac{dt}{ds}$. De otra, si $x(t)$
  es solución de la ecuación

  % WARNING: justificar el uso del teorema de la función inversa
  
  \[
  \begin{cases}
    \dfrac{ds(t)}{dt} = \dfrac{\partial \varphi_1}{\partial t}(t, x(t)) + \dfrac{\partial \varphi_1}{\partial x}(t, x(t))x'(t) = \dfrac{\partial \varphi_1}{\partial t}(t, x(t)) + \dfrac{\partial \varphi_1}{\partial x}(t, x(t))F(t, x) \\ \\

 \dfrac{dy(t)}{dt} = \dfrac{\partial \varphi_2}{\partial t}(t, x(t)) + \dfrac{\partial \varphi_2}{\partial x}(t, x(t))x'(t) = \dfrac{\partial \varphi_2}{\partial t}(t, x(t)) + \dfrac{\partial \varphi_2}{\partial x}(t, x(t))F(t, x)
   
  \end{cases}
  \]

  Con lo cual

  \[
  \dfrac{dy}{ds} = \dfrac{dy}{dt}\dfrac{dt}{ds} = \dfrac{\dfrac{\partial \varphi_1}{\partial t}(t, x(t)) + \dfrac{\partial \varphi_1}{\partial x}(t, x(t))F(t, x)}{\dfrac{\partial \varphi_2}{\partial t}(t, x(t)) + \dfrac{\partial \varphi_2}{\partial x}(t, x(t))F(t, x)}
  = \dfrac{\dfrac{\partial \varphi_1}{\partial t}(\varphi^{-1}(s, y(s))) + \dfrac{\partial \varphi_1}{\partial x}(\varphi^{-1}(s, y(s)))F(\varphi^{-1}(s, y))}{\dfrac{\partial \varphi_2}{\partial t}(\varphi^{-1}(s, y(s))) + \dfrac{\partial \varphi_2}{\partial x}(\varphi^{-1}(s, y(s)))F(\varphi^{-1}(s, y))} = \tilde{F}(s,y)
  \]

  % TODO: insertar ejemplo de uso

\end{proof}

\subsection{Ecuación homogénea.}

Sea $f:(a,b) \to \R$ continua. Tenemos una ecuación diferencial de la forma

\[x' = f\left(\frac{x}{t}\right) \ \implies \ F\left(t,x\right) = f\left(\frac{x}{t}\right)\]

con $t \neq 0, \frac{x}{t} \in (a,b)$.

\[Dom(F) = \left\{(t,x)\in \R ^2 : t \neq 0, \ \frac{x}{t} \in (a,b)\right\}\]

Podemos dividir el dominio de F en dos dominios (abiertos y conexos):\\
\[
\begin{array}{l}
  D_1 = \{(t,x) \in \R ^2 : t > 0, \ at < x < bt\}\\
  D_2 = \{(t,x) \in \R ^2 : t < 0, \ at > x > bt\}
\end{array}
\]

Realizamos un cambio de variable:
\[
\begin{array}{l}
  s = t\\
  y = \dfrac{x}{t}
\end{array}
\]

Los nuevos dominios son:
\[
\begin{array}{l}
  \tilde{D}_1 = \{s > 0, a < y < b\}\\
  \tilde{D}_2 = \{s < 0, a > y > b\}
\end{array}
\]

La ecuación diferencial queda como una ecuación de variables separadas:

$$y' = \frac{1}{s}\left(f(y) - y\right)$$

que sabemos resolver.

\subsubsection{Ecuaciones reducibles a homogéneas}

Tenemos una ecuación diferencial de la forma: 
\[
F(t,x) = f\left( \frac{ax+bt+c}{a'x+b't+c'} \right)
\]

% WARNING: no entiendo por qué si c,c' = 0 es homogénea
Donde si $c,c' = 0$ entonces la ecuación ya es homogénea.

Supongamos que las rectas del plano $ax + bt+c=0$ y $a'x+b't+c'=0$ tienen un punto de corte en $(\alpha, \beta ) \in \R^2$.

Entonces tomamos el cambio de variable:
\begin{align*}
s &= t-\alpha \\
y &= x - \beta
\end{align*}

% TODO: Explicar porque es un difeomorfistmo válido.

Entonces tenemos: 
\[
\frac{dy}{ds} = \frac{dx}{dt} = f\Big( \frac{a(y+\beta ) + b (s+\alpha ) + c}{a'(y+\beta ) + b'(s+ \alpha ) + c'} \Big)
\]
\[
y' = f \bigg( \frac{a \frac{y}{s} + b}{a'\frac{y}{s} + b'} \bigg) = g\Big( \frac{y}{s}\Big)
\]

\subsubsection{La ecuación lineal}

En este caso, tenemos una ecuación diferencial de la forma $F(x,t) = \alpha (t)x + \beta ( t)$ con $\alpha , \beta \in C(a,b)$, donde tenemos que $F:(a,b)\times \R \to \R$ es lineal en x.

Tomamos la ecuación homogénea asociada:
\[
x' = \alpha (t) x
\]
Sabemos que las soluciones de esta ecuación son:
\[
x(t) = A e^{\int_{t_0}^{t} \alpha (s) ds}  \ \forall t \in (a,b), \ A in \R
\]

Supongamos que $\varphi _1$ y $\varphi _2 $ son dos soluciones de la ecuación diferencial inicial en $I$. Entonces, sea $x := \varphi _1  - \varphi _2  \ \implies \ x' = \alpha x$.

Esto quiere decir que sea $\varphi$ una solución, toda solución es $\varphi (t) + A e^{\int_{t_0}^{t} \alpha (s) ds}$.

Eso quiere decir que nuestro problema se reduce a buscar una unica solución, para ello lo que vamos a hacer es que nuestra constante ($A$) dependa de $t$, $\varphi (t) = A(t) e^{\int_{t_0}^{t} \alpha (s) ds}$

% TODO: Completar esto


\begin{nprop}
Sean $P,Q \in C^1(D)$ con $D$ dominio de $\R^2$, si $\exists F\in C^1 (D)$ t.q. $\frac{\partial F}{\partial t} = P \ \ \frac{\partial F}{\partial x} = Q$, entonces, $\frac{\partial P}{\partial x} = \frac{\partial Q}{\partial t}$ en $D$.
\end{nprop}

\begin{proof}
\[
P,Q \in C^1 (D) \ \implies F \in C^2 (D) \ \implies \frac{\partial ^2 F}{\partial t \partial x} = \frac{\partial Q}{\partial t} = \frac{\partial P}{\partial x} = \frac{\partial ^2 F}{\partial x \partial t}
\]
\end{proof}

\begin{ndef}
Se dice que $D \in \R ^n$ es estrellado si $\exists x_0 \in D$ t.q. $\forall x \in D \ [x_0, x] \subset D$
\end{ndef}

\begin{nth}
Sean $P,Q \in C^1(D)$ con $D$ dominio de $\R^2$ y estrellado, con $\frac{\partial P}{\partial x} = \frac{\partial Q}{\partial t}$ en $D$.
Entonces $\exists F \in C^2 (D)$ t.q. $\frac{\partial F}{\partial t} = P$ y $\frac{\partial F}{\partial x} = Q$
\end{nth}

\begin{proof}
Tomamos el punto que hace $D$ estrellado como origen.
\[
F(t,x) := t \int _0 ^1 P(\lambda t, \lambda x) d\lambda + x \int _0 ^1 Q(\lambda t, \lambda x ) d\lambda
\]

La función esta bien definida en las integrales por ser $D$ estrellado.

\[
\frac{\partial F}{\partial t} (t,x) = \int _0 ^1 P(\lambda t, \lambda x) d\lambda + t \int _0 ^1 \lambda \frac{\partial P}{\partial t}(\lambda t, \lambda x) d\lambda + x\int _0 ^1 \lambda \frac{\partial Q}{\partial t}(\lambda t, \lambda x) d\lambda = ... = P
\]
 % TODO: Completar los ...
\[
\frac{\partial F}{\partial x} (t,x) =  ... = Q
\]

\end{proof}

A esta función $F$ se la suele llamar una función potencial $\nabla F = (P,Q)$

\begin{nth}
Sea $g: D\times [a,b]$ de clase 1, tal que: 

\[
(x_1,..., x_n, t) \to g(x_1,... x_n, t)
\]
Sea $f$ una función tal que: 
\[
f(x_1,...,x_n) = \int _a ^b g \ dt
\]
Entonces $f$ es de clase 1 y $\frac{\partial f}{\partial x_i} = \int _a ^b \frac{\partial g}{\partial x_i}$


\end{nth}
\begin{ndef}
Sean $P,Q \in C^1 (D)$ considero la ecuación $P(t,x) + Q(t,x) x' = 0$. Se dice que es exacta si $\frac{\partial P}{\partial x} = \frac{\partial Q}{\partial t}$ en $D$.
\end{ndef}

Supongamos una ecuación como la de la proposición anterior, con $(t_0, x_0) \in D$ t.q. $Q(t_0, x_0) \neq 0$. Entonces $\exists \varepsilon > 0$ t.q. $B  := B( (t_0 x_0 ), \varepsilon) \subset D$ es un subconjunto estrellado $\implies \ \exists F \in C^2 (B)$ t.q. $\frac{\partial F}{\partial t} = P$ y $\frac{\partial F}{\partial x} = Q$

Entonces considero  $F(x,t) = F(x_0, t_0)$ que define $x=x(t)$ en un entorno $I$ de $t_0$

% TODO: Ejemplo de uso

\begin{ndef}
Sean $P,Q \in C^1 (D)$ con $P(t,x) + Q(t,x) x' = 0$ no exacta. Se dice que $\mu (t,x)$ es un factor integrante para la ecuación si:
\begin{enumerate}
\item $\mu(t,x) \neq 0 \ \forall (t,x) \in D$
\item $\mu \in C^1 (D)$
\item $\mu (t,x)P(t,x) + \mu (t,x)Q(t,x) x' = 0$ es una ecuación exacta.

Nota: Siempre existe un factor integrante.
\end{enumerate}

\end{ndef}



